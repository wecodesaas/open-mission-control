"""
Parallel Follow-up PR Reviewer
===============================

PR follow-up reviewer using Claude Agent SDK subagents for parallel specialist analysis.

The orchestrator analyzes incremental changes and delegates to specialized agents:
- resolution-verifier: Verifies previous findings are addressed
- new-code-reviewer: Reviews new code for issues
- comment-analyzer: Processes contributor and AI feedback

Key Design:
- AI decides which agents to invoke (NOT programmatic rules)
- Subagents defined via SDK `agents={}` parameter
- SDK handles parallel execution automatically
- User-configured model from frontend settings (no hardcoding)
"""

from __future__ import annotations

import hashlib
import logging
import os
import shutil
import subprocess
import uuid
from pathlib import Path
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from ..models import FollowupReviewContext

from claude_agent_sdk import AgentDefinition

try:
    from ...core.client import create_client
    from ...phase_config import get_thinking_budget
    from ..context_gatherer import _validate_git_ref
    from ..gh_client import GHClient
    from ..models import (
        GitHubRunnerConfig,
        MergeVerdict,
        PRReviewFinding,
        PRReviewResult,
        ReviewSeverity,
    )
    from .category_utils import map_category
    from .pydantic_models import ParallelFollowupResponse
    from .sdk_utils import process_sdk_stream
except (ImportError, ValueError, SystemError):
    from context_gatherer import _validate_git_ref
    from core.client import create_client
    from gh_client import GHClient
    from models import (
        GitHubRunnerConfig,
        MergeVerdict,
        PRReviewFinding,
        PRReviewResult,
        ReviewSeverity,
    )
    from phase_config import get_thinking_budget
    from services.category_utils import map_category
    from services.pydantic_models import ParallelFollowupResponse
    from services.sdk_utils import process_sdk_stream


logger = logging.getLogger(__name__)

# Check if debug mode is enabled
DEBUG_MODE = os.environ.get("DEBUG", "").lower() in ("true", "1", "yes")

# Directory for PR review worktrees (shared with initial reviewer)
PR_WORKTREE_DIR = ".auto-claude/github/pr/worktrees"

# Severity mapping for AI responses
_SEVERITY_MAPPING = {
    "critical": ReviewSeverity.CRITICAL,
    "high": ReviewSeverity.HIGH,
    "medium": ReviewSeverity.MEDIUM,
    "low": ReviewSeverity.LOW,
}


def _map_severity(severity_str: str) -> ReviewSeverity:
    """Map severity string to ReviewSeverity enum."""
    return _SEVERITY_MAPPING.get(severity_str.lower(), ReviewSeverity.MEDIUM)


class ParallelFollowupReviewer:
    """
    Follow-up PR reviewer using SDK subagents for parallel specialist analysis.

    The orchestrator:
    1. Analyzes incremental changes since last review
    2. Delegates to appropriate specialist agents (SDK handles parallel execution)
    3. Synthesizes findings into a final merge verdict

    Specialist Agents:
    - resolution-verifier: Verifies previous findings are addressed
    - new-code-reviewer: Reviews new code for issues
    - comment-analyzer: Processes contributor and AI feedback

    Model Configuration:
    - Orchestrator uses user-configured model from frontend settings
    - Specialist agents use model="inherit" (same as orchestrator)
    """

    def __init__(
        self,
        project_dir: Path,
        github_dir: Path,
        config: GitHubRunnerConfig,
        progress_callback=None,
    ):
        self.project_dir = Path(project_dir)
        self.github_dir = Path(github_dir)
        self.config = config
        self.progress_callback = progress_callback

    def _report_progress(self, phase: str, progress: int, message: str, **kwargs):
        """Report progress if callback is set."""
        if self.progress_callback:
            import sys

            if "orchestrator" in sys.modules:
                ProgressCallback = sys.modules["orchestrator"].ProgressCallback
            else:
                try:
                    from ..orchestrator import ProgressCallback
                except ImportError:
                    from orchestrator import ProgressCallback

            self.progress_callback(
                ProgressCallback(
                    phase=phase, progress=progress, message=message, **kwargs
                )
            )

    def _load_prompt(self, filename: str) -> str:
        """Load a prompt file from the prompts/github directory."""
        prompt_file = (
            Path(__file__).parent.parent.parent.parent / "prompts" / "github" / filename
        )
        if prompt_file.exists():
            return prompt_file.read_text(encoding="utf-8")
        logger.warning(f"Prompt file not found: {prompt_file}")
        return ""

    def _create_pr_worktree(self, head_sha: str, pr_number: int) -> Path:
        """Create a temporary worktree at the PR head commit.

        Args:
            head_sha: The commit SHA of the PR head (validated before use)
            pr_number: The PR number for naming

        Returns:
            Path to the created worktree

        Raises:
            RuntimeError: If worktree creation fails
            ValueError: If head_sha fails validation (command injection prevention)
        """
        # SECURITY: Validate git ref before use in subprocess calls
        if not _validate_git_ref(head_sha):
            raise ValueError(
                f"Invalid git ref: '{head_sha}'. "
                "Must contain only alphanumeric characters, dots, slashes, underscores, and hyphens."
            )

        worktree_name = f"pr-followup-{pr_number}-{uuid.uuid4().hex[:8]}"
        worktree_dir = self.project_dir / PR_WORKTREE_DIR

        if DEBUG_MODE:
            print(f"[Followup] DEBUG: project_dir={self.project_dir}", flush=True)
            print(f"[Followup] DEBUG: worktree_dir={worktree_dir}", flush=True)
            print(f"[Followup] DEBUG: head_sha={head_sha}", flush=True)

        worktree_dir.mkdir(parents=True, exist_ok=True)
        worktree_path = worktree_dir / worktree_name

        if DEBUG_MODE:
            print(f"[Followup] DEBUG: worktree_path={worktree_path}", flush=True)

        # Fetch the commit if not available locally (handles fork PRs)
        fetch_result = subprocess.run(
            ["git", "fetch", "origin", head_sha],
            cwd=self.project_dir,
            capture_output=True,
            text=True,
            timeout=60,
        )
        if DEBUG_MODE:
            print(
                f"[Followup] DEBUG: fetch returncode={fetch_result.returncode}",
                flush=True,
            )

        # Create detached worktree at the PR commit
        result = subprocess.run(
            ["git", "worktree", "add", "--detach", str(worktree_path), head_sha],
            cwd=self.project_dir,
            capture_output=True,
            text=True,
            timeout=120,
        )

        if DEBUG_MODE:
            print(
                f"[Followup] DEBUG: worktree add returncode={result.returncode}",
                flush=True,
            )
            if result.stderr:
                print(
                    f"[Followup] DEBUG: worktree add stderr={result.stderr[:200]}",
                    flush=True,
                )

        if result.returncode != 0:
            raise RuntimeError(f"Failed to create worktree: {result.stderr}")

        logger.info(f"[Followup] Created worktree at {worktree_path}")
        return worktree_path

    def _cleanup_pr_worktree(self, worktree_path: Path) -> None:
        """Remove a temporary PR review worktree with fallback chain.

        Args:
            worktree_path: Path to the worktree to remove
        """
        if not worktree_path or not worktree_path.exists():
            return

        if DEBUG_MODE:
            print(
                f"[Followup] DEBUG: Cleaning up worktree at {worktree_path}",
                flush=True,
            )

        # Try 1: git worktree remove
        result = subprocess.run(
            ["git", "worktree", "remove", "--force", str(worktree_path)],
            cwd=self.project_dir,
            capture_output=True,
            text=True,
            timeout=30,
        )

        if result.returncode == 0:
            logger.info(f"[Followup] Cleaned up worktree: {worktree_path.name}")
            return

        # Try 2: shutil.rmtree fallback
        try:
            shutil.rmtree(worktree_path, ignore_errors=True)
            subprocess.run(
                ["git", "worktree", "prune"],
                cwd=self.project_dir,
                capture_output=True,
                timeout=30,
            )
            logger.warning(f"[Followup] Used shutil fallback for: {worktree_path.name}")
        except Exception as e:
            logger.error(f"[Followup] Failed to cleanup worktree {worktree_path}: {e}")

    def _define_specialist_agents(self) -> dict[str, AgentDefinition]:
        """
        Define specialist agents for follow-up review.

        Each agent has:
        - description: When the orchestrator should invoke this agent
        - prompt: System prompt for the agent
        - tools: Tools the agent can use (read-only for PR review)
        - model: "inherit" = use same model as orchestrator (user's choice)
        """
        # Load agent prompts from files
        resolution_prompt = self._load_prompt("pr_followup_resolution_agent.md")
        newcode_prompt = self._load_prompt("pr_followup_newcode_agent.md")
        comment_prompt = self._load_prompt("pr_followup_comment_agent.md")
        validator_prompt = self._load_prompt("pr_finding_validator.md")

        return {
            "resolution-verifier": AgentDefinition(
                description=(
                    "Resolution verification specialist. Use to verify whether previous "
                    "findings have been addressed. Analyzes diffs to determine if issues "
                    "are truly fixed, partially fixed, or still unresolved. "
                    "Invoke when: There are previous findings to verify."
                ),
                prompt=resolution_prompt
                or "You verify whether previous findings are resolved.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
            "new-code-reviewer": AgentDefinition(
                description=(
                    "New code analysis specialist. Reviews code added since last review "
                    "for security, logic, quality issues, and regressions. "
                    "Invoke when: There are substantial code changes (>50 lines diff) or "
                    "changes to security-sensitive areas."
                ),
                prompt=newcode_prompt or "You review new code for issues.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
            "comment-analyzer": AgentDefinition(
                description=(
                    "Comment and feedback analyst. Processes contributor comments and "
                    "AI tool reviews (CodeRabbit, Cursor, Gemini, etc.) to identify "
                    "unanswered questions and valid concerns. "
                    "Invoke when: There are comments or formal reviews since last review."
                ),
                prompt=comment_prompt or "You analyze comments and feedback.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
            "finding-validator": AgentDefinition(
                description=(
                    "Finding re-investigation specialist. Re-investigates unresolved findings "
                    "to validate they are actually real issues, not false positives. "
                    "Actively reads the code at the finding location with fresh eyes. "
                    "Can confirm findings as valid OR dismiss them as false positives. "
                    "CRITICAL: Invoke for ALL unresolved findings after resolution-verifier runs. "
                    "Invoke when: There are findings marked as unresolved that need validation."
                ),
                prompt=validator_prompt
                or "You validate whether unresolved findings are real issues.",
                tools=["Read", "Grep", "Glob"],
                model="inherit",
            ),
        }

    def _format_previous_findings(self, context: FollowupReviewContext) -> str:
        """Format previous findings for the prompt."""
        previous_findings = context.previous_review.findings
        if not previous_findings:
            return "No previous findings to verify."

        lines = []
        for f in previous_findings:
            lines.append(
                f"- **{f.id}** [{f.severity.value}] {f.title}\n"
                f"  File: {f.file}:{f.line}\n"
                f"  {f.description[:200]}..."
            )
        return "\n".join(lines)

    def _format_commits(self, context: FollowupReviewContext) -> str:
        """Format new commits for the prompt."""
        if not context.commits_since_review:
            return "No new commits."

        lines = []
        for commit in context.commits_since_review[:20]:  # Limit to 20 commits
            sha = commit.get("sha", "")[:7]
            message = commit.get("commit", {}).get("message", "").split("\n")[0]
            author = commit.get("commit", {}).get("author", {}).get("name", "unknown")
            lines.append(f"- `{sha}` by {author}: {message}")
        return "\n".join(lines)

    def _format_comments(self, context: FollowupReviewContext) -> str:
        """Format contributor comments for the prompt."""
        if not context.contributor_comments_since_review:
            return "No contributor comments since last review."

        lines = []
        for comment in context.contributor_comments_since_review[:15]:
            author = comment.get("user", {}).get("login", "unknown")
            body = comment.get("body", "")[:300]
            lines.append(f"**@{author}**: {body}")
        return "\n\n".join(lines)

    def _format_ai_reviews(self, context: FollowupReviewContext) -> str:
        """Format AI bot reviews and comments for the prompt."""
        ai_content = []

        # AI bot comments
        for comment in context.ai_bot_comments_since_review[:10]:
            author = comment.get("user", {}).get("login", "unknown")
            body = comment.get("body", "")[:500]
            ai_content.append(f"**{author}** (comment):\n{body}")

        # Formal PR reviews from AI tools
        for review in context.pr_reviews_since_review[:5]:
            author = review.get("user", {}).get("login", "unknown")
            body = review.get("body", "")[:1000]
            state = review.get("state", "unknown")
            ai_content.append(f"**{author}** ({state}):\n{body}")

        if not ai_content:
            return "No AI tool feedback since last review."

        return "\n\n---\n\n".join(ai_content)

    def _format_ci_status(self, context: FollowupReviewContext) -> str:
        """Format CI status for the prompt."""
        ci_status = context.ci_status
        if not ci_status:
            return "CI status not available."

        passing = ci_status.get("passing", 0)
        failing = ci_status.get("failing", 0)
        pending = ci_status.get("pending", 0)
        failed_checks = ci_status.get("failed_checks", [])
        awaiting_approval = ci_status.get("awaiting_approval", 0)

        lines = []

        # Overall status
        if failing > 0:
            lines.append(f"âš ï¸ **{failing} CI check(s) FAILING** - PR cannot be merged")
        elif pending > 0:
            lines.append(f"â³ **{pending} CI check(s) pending** - Wait for completion")
        elif passing > 0:
            lines.append(f"âœ… **All {passing} CI check(s) passing**")
        else:
            lines.append("No CI checks configured")

        # List failed checks
        if failed_checks:
            lines.append("\n**Failed checks:**")
            for check in failed_checks:
                lines.append(f"  - âŒ {check}")

        # Awaiting approval (fork PRs)
        if awaiting_approval > 0:
            lines.append(
                f"\nâ¸ï¸ **{awaiting_approval} workflow(s) awaiting maintainer approval** (fork PR)"
            )

        return "\n".join(lines)

    def _build_orchestrator_prompt(self, context: FollowupReviewContext) -> str:
        """Build full prompt for orchestrator with follow-up context."""
        # Load orchestrator prompt
        base_prompt = self._load_prompt("pr_followup_orchestrator.md")
        if not base_prompt:
            base_prompt = "You are a follow-up PR reviewer. Verify resolutions and find new issues."

        # Build context sections
        previous_findings = self._format_previous_findings(context)
        commits = self._format_commits(context)
        contributor_comments = self._format_comments(context)
        ai_reviews = self._format_ai_reviews(context)
        ci_status = self._format_ci_status(context)

        # Truncate diff if too long
        MAX_DIFF_CHARS = 100_000
        diff_content = context.diff_since_review
        if len(diff_content) > MAX_DIFF_CHARS:
            diff_content = diff_content[:MAX_DIFF_CHARS] + "\n\n... (diff truncated)"

        followup_context = f"""
---

## Follow-up Review Context

**PR Number:** {context.pr_number}
**Previous Review Commit:** {context.previous_commit_sha[:8]}
**Current HEAD:** {context.current_commit_sha[:8]}
**New Commits:** {len(context.commits_since_review)}
**Files Changed:** {len(context.files_changed_since_review)}

### CI Status (CRITICAL - Must Factor Into Verdict)
{ci_status}

### Previous Review Summary
{context.previous_review.summary[:500] if context.previous_review.summary else "No summary available."}

### Previous Findings to Verify
{previous_findings}

### New Commits Since Last Review
{commits}

### Files Changed Since Last Review
{chr(10).join(f"- {f}" for f in context.files_changed_since_review[:30])}

### Contributor Comments Since Last Review
{contributor_comments}

### AI Tool Feedback Since Last Review
{ai_reviews}

### Diff Since Last Review
```diff
{diff_content}
```

---

Now analyze this follow-up and delegate to the appropriate specialist agents.
Remember: YOU decide which agents to invoke based on YOUR analysis.
The SDK will run invoked agents in parallel automatically.
**CRITICAL: Your verdict MUST account for CI status. Failing CI = BLOCKED verdict.**
"""

        return base_prompt + followup_context

    async def review(self, context: FollowupReviewContext) -> PRReviewResult:
        """
        Main follow-up review entry point.

        Args:
            context: Follow-up context with incremental changes

        Returns:
            PRReviewResult with findings and verdict
        """
        logger.info(
            f"[ParallelFollowup] Starting follow-up review for PR #{context.pr_number}"
        )

        # Track worktree for cleanup
        worktree_path: Path | None = None

        try:
            self._report_progress(
                "orchestrating",
                35,
                "Parallel orchestrator analyzing follow-up...",
                pr_number=context.pr_number,
            )

            # Build orchestrator prompt
            prompt = self._build_orchestrator_prompt(context)

            # Get project root - default to local checkout
            project_root = (
                self.project_dir.parent.parent
                if self.project_dir.name == "backend"
                else self.project_dir
            )

            # Create temporary worktree at PR head commit for isolated review
            # This ensures agents read from the correct PR state, not the current checkout
            head_sha = context.current_commit_sha
            if head_sha and _validate_git_ref(head_sha):
                try:
                    if DEBUG_MODE:
                        print(
                            f"[Followup] DEBUG: Creating worktree for head_sha={head_sha}",
                            flush=True,
                        )
                    worktree_path = self._create_pr_worktree(
                        head_sha, context.pr_number
                    )
                    project_root = worktree_path
                    print(
                        f"[Followup] Using worktree at {worktree_path.name} for PR review",
                        flush=True,
                    )
                except Exception as e:
                    if DEBUG_MODE:
                        print(
                            f"[Followup] DEBUG: Worktree creation FAILED: {e}",
                            flush=True,
                        )
                    logger.warning(
                        f"[ParallelFollowup] Worktree creation failed, "
                        f"falling back to local checkout: {e}"
                    )
                    # Fallback to original behavior if worktree creation fails
            else:
                logger.warning(
                    f"[ParallelFollowup] Invalid or missing head_sha '{head_sha}', "
                    "using local checkout"
                )

            # Use model and thinking level from config (user settings)
            model = self.config.model or "claude-sonnet-4-5-20250929"
            thinking_level = self.config.thinking_level or "medium"
            thinking_budget = get_thinking_budget(thinking_level)

            logger.info(
                f"[ParallelFollowup] Using model={model}, "
                f"thinking_level={thinking_level}, thinking_budget={thinking_budget}"
            )

            # Create client with subagents defined
            client = create_client(
                project_dir=project_root,
                spec_dir=self.github_dir,
                model=model,
                agent_type="pr_followup_parallel",
                max_thinking_tokens=thinking_budget,
                agents=self._define_specialist_agents(),
                output_format={
                    "type": "json_schema",
                    "schema": ParallelFollowupResponse.model_json_schema(),
                },
            )

            self._report_progress(
                "orchestrating",
                40,
                "Orchestrator delegating to specialist agents...",
                pr_number=context.pr_number,
            )

            # Run orchestrator session using shared SDK stream processor
            async with client:
                await client.query(prompt)

                print(
                    f"[ParallelFollowup] Running orchestrator ({model})...",
                    flush=True,
                )

                # Process SDK stream with shared utility
                stream_result = await process_sdk_stream(
                    client=client,
                    context_name="ParallelFollowup",
                )

                # Check for stream processing errors
                if stream_result.get("error"):
                    logger.error(
                        f"[ParallelFollowup] SDK stream failed: {stream_result['error']}"
                    )
                    raise RuntimeError(
                        f"SDK stream processing failed: {stream_result['error']}"
                    )

                result_text = stream_result["result_text"]
                structured_output = stream_result["structured_output"]
                agents_invoked = stream_result["agents_invoked"]
                msg_count = stream_result["msg_count"]

            self._report_progress(
                "finalizing",
                50,
                "Synthesizing follow-up findings...",
                pr_number=context.pr_number,
            )

            # Parse findings from output
            if structured_output:
                result_data = self._parse_structured_output(structured_output, context)
            else:
                result_data = self._parse_text_output(result_text, context)

            # Extract data
            findings = result_data.get("findings", [])
            resolved_ids = result_data.get("resolved_ids", [])
            unresolved_ids = result_data.get("unresolved_ids", [])
            new_finding_ids = result_data.get("new_finding_ids", [])
            verdict = result_data.get("verdict", MergeVerdict.NEEDS_REVISION)
            verdict_reasoning = result_data.get("verdict_reasoning", "")

            # Use agents from structured output (more reliable than streaming detection)
            agents_from_result = result_data.get("agents_invoked", [])
            final_agents = agents_from_result if agents_from_result else agents_invoked
            logger.info(
                f"[ParallelFollowup] Session complete. Agents invoked: {final_agents}"
            )
            print(
                f"[ParallelFollowup] Complete. Agents invoked: {final_agents}",
                flush=True,
            )

            # Deduplicate findings
            unique_findings = self._deduplicate_findings(findings)

            logger.info(
                f"[ParallelFollowup] Review complete: {len(unique_findings)} findings, "
                f"{len(resolved_ids)} resolved, {len(unresolved_ids)} unresolved"
            )

            # Extract validation counts
            dismissed_count = len(result_data.get("dismissed_false_positive_ids", []))
            confirmed_count = result_data.get("confirmed_valid_count", 0)
            needs_human_count = result_data.get("needs_human_review_count", 0)

            # Generate summary
            summary = self._generate_summary(
                verdict=verdict,
                verdict_reasoning=verdict_reasoning,
                resolved_count=len(resolved_ids),
                unresolved_count=len(unresolved_ids),
                new_count=len(new_finding_ids),
                agents_invoked=final_agents,
                dismissed_false_positive_count=dismissed_count,
                confirmed_valid_count=confirmed_count,
                needs_human_review_count=needs_human_count,
            )

            # Map verdict to overall_status
            if verdict == MergeVerdict.BLOCKED:
                overall_status = "request_changes"
            elif verdict == MergeVerdict.NEEDS_REVISION:
                overall_status = "request_changes"
            elif verdict == MergeVerdict.MERGE_WITH_CHANGES:
                overall_status = "comment"
            else:
                overall_status = "approve"

            # Generate blockers from critical/high/medium severity findings
            # (Medium also blocks merge in our strict quality gates approach)
            blockers = []
            for finding in unique_findings:
                if finding.severity in (
                    ReviewSeverity.CRITICAL,
                    ReviewSeverity.HIGH,
                    ReviewSeverity.MEDIUM,
                ):
                    blockers.append(f"{finding.category.value}: {finding.title}")

            # Get file blob SHAs for rebase-resistant follow-up reviews
            # Blob SHAs persist across rebases - same content = same blob SHA
            file_blobs: dict[str, str] = {}
            try:
                gh_client = GHClient(
                    project_dir=self.project_dir,
                    default_timeout=30.0,
                    repo=self.config.repo,
                )
                pr_files = await gh_client.get_pr_files(context.pr_number)
                for file in pr_files:
                    filename = file.get("filename", "")
                    blob_sha = file.get("sha", "")
                    if filename and blob_sha:
                        file_blobs[filename] = blob_sha
                logger.info(
                    f"Captured {len(file_blobs)} file blob SHAs for follow-up tracking"
                )
            except Exception as e:
                logger.warning(f"Could not capture file blobs: {e}")

            result = PRReviewResult(
                pr_number=context.pr_number,
                repo=self.config.repo,
                success=True,
                findings=unique_findings,
                summary=summary,
                overall_status=overall_status,
                verdict=verdict,
                verdict_reasoning=verdict_reasoning,
                blockers=blockers,
                reviewed_commit_sha=context.current_commit_sha,
                reviewed_file_blobs=file_blobs,
                is_followup_review=True,
                previous_review_id=context.previous_review.review_id
                or context.previous_review.pr_number,
                resolved_findings=resolved_ids,
                unresolved_findings=unresolved_ids,
                new_findings_since_last_review=new_finding_ids,
            )

            self._report_progress(
                "analyzed",
                60,
                "Follow-up analysis complete",
                pr_number=context.pr_number,
            )

            return result

        except Exception as e:
            logger.error(f"[ParallelFollowup] Review failed: {e}", exc_info=True)
            print(f"[ParallelFollowup] Error: {e}", flush=True)

            return PRReviewResult(
                pr_number=context.pr_number,
                repo=self.config.repo,
                success=False,
                findings=[],
                summary=f"Follow-up review failed: {e}",
                overall_status="comment",
                verdict=MergeVerdict.NEEDS_REVISION,
                verdict_reasoning=f"Review failed: {e}",
                blockers=[str(e)],
                is_followup_review=True,
                reviewed_commit_sha=context.current_commit_sha,
            )
        finally:
            # Always cleanup worktree, even on error
            if worktree_path:
                self._cleanup_pr_worktree(worktree_path)

    def _parse_structured_output(
        self, data: dict, context: FollowupReviewContext
    ) -> dict:
        """Parse structured output from ParallelFollowupResponse."""
        try:
            # Validate with Pydantic
            response = ParallelFollowupResponse.model_validate(data)

            # Log agents from structured output
            agents_from_output = response.agents_invoked or []
            if agents_from_output:
                print(
                    f"[ParallelFollowup] Specialist agents invoked: {', '.join(agents_from_output)}",
                    flush=True,
                )
                for agent in agents_from_output:
                    print(f"[Agent:{agent}] Analysis complete", flush=True)

            findings = []
            resolved_ids = []
            unresolved_ids = []
            new_finding_ids = []

            # Process resolution verifications
            # First, build a map of finding validations (from finding-validator agent)
            validation_map = {}
            dismissed_ids = []
            for fv in response.finding_validations:
                validation_map[fv.finding_id] = fv
                if fv.validation_status == "dismissed_false_positive":
                    dismissed_ids.append(fv.finding_id)
                    print(
                        f"[ParallelFollowup] Finding {fv.finding_id} DISMISSED as false positive: {fv.explanation[:100]}",
                        flush=True,
                    )

            for rv in response.resolution_verifications:
                if rv.status == "resolved":
                    resolved_ids.append(rv.finding_id)
                elif rv.status in ("unresolved", "partially_resolved", "cant_verify"):
                    # Check if finding was validated and dismissed as false positive
                    if rv.finding_id in dismissed_ids:
                        # Finding-validator determined this was a false positive - skip it
                        print(
                            f"[ParallelFollowup] Skipping {rv.finding_id} - dismissed as false positive by finding-validator",
                            flush=True,
                        )
                        resolved_ids.append(
                            rv.finding_id
                        )  # Count as resolved (false positive)
                        continue

                    # Include "cant_verify" as unresolved - if we can't verify, assume not fixed
                    unresolved_ids.append(rv.finding_id)
                    # Add unresolved as a finding
                    if rv.status in ("unresolved", "cant_verify"):
                        # Find original finding
                        original = next(
                            (
                                f
                                for f in context.previous_review.findings
                                if f.id == rv.finding_id
                            ),
                            None,
                        )
                        if original:
                            # Check if we have validation evidence
                            validation = validation_map.get(rv.finding_id)
                            validation_status = None
                            validation_evidence = None
                            validation_explanation = None

                            if validation:
                                validation_status = validation.validation_status
                                validation_evidence = validation.code_evidence
                                validation_explanation = validation.explanation

                            findings.append(
                                PRReviewFinding(
                                    id=rv.finding_id,
                                    severity=original.severity,
                                    category=original.category,
                                    title=f"[UNRESOLVED] {original.title}",
                                    description=f"{original.description}\n\nResolution note: {rv.evidence}",
                                    file=original.file,
                                    line=original.line,
                                    suggested_fix=original.suggested_fix,
                                    fixable=original.fixable,
                                    validation_status=validation_status,
                                    validation_evidence=validation_evidence,
                                    validation_explanation=validation_explanation,
                                )
                            )

            # Process new findings
            for nf in response.new_findings:
                finding_id = nf.id or self._generate_finding_id(
                    nf.file, nf.line, nf.title
                )
                new_finding_ids.append(finding_id)
                findings.append(
                    PRReviewFinding(
                        id=finding_id,
                        severity=_map_severity(nf.severity),
                        category=map_category(nf.category),
                        title=nf.title,
                        description=nf.description,
                        file=nf.file,
                        line=nf.line,
                        suggested_fix=nf.suggested_fix,
                        fixable=nf.fixable,
                    )
                )

            # Process comment findings
            for cf in response.comment_findings:
                finding_id = cf.id or self._generate_finding_id(
                    cf.file, cf.line, cf.title
                )
                new_finding_ids.append(finding_id)
                findings.append(
                    PRReviewFinding(
                        id=finding_id,
                        severity=_map_severity(cf.severity),
                        category=map_category(cf.category),
                        title=f"[FROM COMMENTS] {cf.title}",
                        description=cf.description,
                        file=cf.file,
                        line=cf.line,
                        suggested_fix=cf.suggested_fix,
                        fixable=cf.fixable,
                    )
                )

            # Map verdict
            verdict_map = {
                "READY_TO_MERGE": MergeVerdict.READY_TO_MERGE,
                "MERGE_WITH_CHANGES": MergeVerdict.MERGE_WITH_CHANGES,
                "NEEDS_REVISION": MergeVerdict.NEEDS_REVISION,
                "BLOCKED": MergeVerdict.BLOCKED,
            }
            verdict = verdict_map.get(response.verdict, MergeVerdict.NEEDS_REVISION)

            # Count validation results
            confirmed_valid_count = sum(
                1
                for fv in response.finding_validations
                if fv.validation_status == "confirmed_valid"
            )
            needs_human_count = sum(
                1
                for fv in response.finding_validations
                if fv.validation_status == "needs_human_review"
            )

            # Log findings summary for verification
            print(
                f"[ParallelFollowup] Parsed {len(findings)} findings, "
                f"{len(resolved_ids)} resolved, {len(unresolved_ids)} unresolved, "
                f"{len(new_finding_ids)} new",
                flush=True,
            )
            if dismissed_ids:
                print(
                    f"[ParallelFollowup] Validation: {len(dismissed_ids)} findings dismissed as false positives, "
                    f"{confirmed_valid_count} confirmed valid, {needs_human_count} need human review",
                    flush=True,
                )
            if findings:
                print("[ParallelFollowup] Findings summary:", flush=True)
                for i, f in enumerate(findings, 1):
                    validation_note = ""
                    if f.validation_status == "confirmed_valid":
                        validation_note = " [VALIDATED]"
                    elif f.validation_status == "needs_human_review":
                        validation_note = " [NEEDS HUMAN REVIEW]"
                    print(
                        f"  [{f.severity.value.upper()}] {i}. {f.title} ({f.file}:{f.line}){validation_note}",
                        flush=True,
                    )

            return {
                "findings": findings,
                "resolved_ids": resolved_ids,
                "unresolved_ids": unresolved_ids,
                "new_finding_ids": new_finding_ids,
                "dismissed_false_positive_ids": dismissed_ids,
                "confirmed_valid_count": confirmed_valid_count,
                "needs_human_review_count": needs_human_count,
                "verdict": verdict,
                "verdict_reasoning": response.verdict_reasoning,
                "agents_invoked": agents_from_output,
            }

        except Exception as e:
            logger.warning(f"[ParallelFollowup] Failed to parse structured output: {e}")
            return self._create_empty_result()

    def _parse_text_output(self, text: str, context: FollowupReviewContext) -> dict:
        """Parse text output when structured output fails."""
        logger.warning("[ParallelFollowup] Falling back to text parsing")

        # Simple heuristic parsing
        findings = []

        # Look for verdict keywords
        text_lower = text.lower()
        if "ready to merge" in text_lower or "approve" in text_lower:
            verdict = MergeVerdict.READY_TO_MERGE
        elif "blocked" in text_lower or "critical" in text_lower:
            verdict = MergeVerdict.BLOCKED
        elif "needs revision" in text_lower or "request changes" in text_lower:
            verdict = MergeVerdict.NEEDS_REVISION
        else:
            verdict = MergeVerdict.MERGE_WITH_CHANGES

        return {
            "findings": findings,
            "resolved_ids": [],
            "unresolved_ids": [],
            "new_finding_ids": [],
            "verdict": verdict,
            "verdict_reasoning": text[:500] if text else "Unable to parse response",
        }

    def _create_empty_result(self) -> dict:
        """Create empty result structure."""
        return {
            "findings": [],
            "resolved_ids": [],
            "unresolved_ids": [],
            "new_finding_ids": [],
            "verdict": MergeVerdict.NEEDS_REVISION,
            "verdict_reasoning": "Unable to parse review results",
        }

    def _generate_finding_id(self, file: str, line: int, title: str) -> str:
        """Generate a unique finding ID."""
        content = f"{file}:{line}:{title}"
        return f"FU-{hashlib.md5(content.encode(), usedforsecurity=False).hexdigest()[:8].upper()}"

    def _deduplicate_findings(
        self, findings: list[PRReviewFinding]
    ) -> list[PRReviewFinding]:
        """Remove duplicate findings."""
        seen = set()
        unique = []
        for f in findings:
            key = (f.file, f.line, f.title.lower().strip())
            if key not in seen:
                seen.add(key)
                unique.append(f)
        return unique

    def _generate_summary(
        self,
        verdict: MergeVerdict,
        verdict_reasoning: str,
        resolved_count: int,
        unresolved_count: int,
        new_count: int,
        agents_invoked: list[str],
        dismissed_false_positive_count: int = 0,
        confirmed_valid_count: int = 0,
        needs_human_review_count: int = 0,
    ) -> str:
        """Generate a human-readable summary of the follow-up review."""
        status_emoji = {
            MergeVerdict.READY_TO_MERGE: "âœ…",
            MergeVerdict.MERGE_WITH_CHANGES: "âš ï¸",
            MergeVerdict.NEEDS_REVISION: "ğŸ”„",
            MergeVerdict.BLOCKED: "ğŸš«",
        }

        emoji = status_emoji.get(verdict, "ğŸ“")
        agents_str = (
            ", ".join(agents_invoked) if agents_invoked else "orchestrator only"
        )

        # Build validation section if there are validation results
        validation_section = ""
        if (
            dismissed_false_positive_count > 0
            or confirmed_valid_count > 0
            or needs_human_review_count > 0
        ):
            validation_section = f"""
### Finding Validation
- ğŸ” **Dismissed as False Positives**: {dismissed_false_positive_count} findings were re-investigated and found to be incorrect
- âœ“ **Confirmed Valid**: {confirmed_valid_count} findings verified as genuine issues
- ğŸ‘¤ **Needs Human Review**: {needs_human_review_count} findings require manual verification
"""

        summary = f"""## {emoji} Follow-up Review: {verdict.value.replace("_", " ").title()}

### Resolution Status
- âœ… **Resolved**: {resolved_count} previous findings addressed
- âŒ **Unresolved**: {unresolved_count} previous findings remain
- ğŸ†• **New Issues**: {new_count} new findings in recent changes
{validation_section}
### Verdict
{verdict_reasoning}

### Review Process
Agents invoked: {agents_str}

---
*This is an AI-generated follow-up review using parallel specialist analysis with finding validation.*
"""
        return summary
